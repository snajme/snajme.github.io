---
title: "Effective NLP ML pipelines"
date: 2023-08-02T17:24:27+03:30
draft: True
---

# A Look genetic chatbot assistant for Biomedical Engineers Researchers: What encompasses an Effective NLP ML pipeline? 

One of the main important features of a reliable ML NLP pipeline is that it should incorporate the elements of  MLOps. NLP pipelines are a series of interconnected data-processing components that work together
analyze natural language data using machine learning techniques. 

There have been ideas and real-world examples around chatbots that can go into a full interaction with the DNA test result receiver. The genetic conversational chatbots are capable of analyzing the likelihood of severe illness and even passing both positive and negative traits through genetics to the next generation. 
What is more compelling is the idea having a chatbot to assist the chief scientists while conducting ground breaking researches. So far we have seen start-ups such as clear genetics which launched to solely help the patient with their DNA test results. 


### What does pipeline mean in the context of natural language processing? 

This included several stages. The initialization of the pipeline commences with the data collection. [talk about advanced data collection techniques in NLP]
The follow-up to this process is the preprocessing stage [change the word here]. 
Researchers need to be able to recruit large and often specialized crowdsourcing of the data. This often happens in categorical data acquisition. This should be done with as low expenditure as possible. 
For Example, in the context of medicine, the conventional method of data acquisition can be both time and money-consuming. [A system that data is contributed freely]
one method is to practice advertising small self-contained tasks on the web. Besides this in medical NLP data gathering the answer should be valid and high quality.  
A method to collect medical data is to leverage Amazon Turk crowdsourcing tools but this tool is highly beneficial for nonspecific normative datasets.  
with Amazon Turk crowdsourcing tools the demographic can be limiting. 


Another example similar to this method of NLP data gathering is the use of **Azure Data Factory**. 


### just note-taking 

Remote surveys for medical natural language processing are costly, especially if the demand is for a new domain in the medical spectrum. The process of collecting natural language processing medical knowledge requires expanding and transforming the questions when the group of researchers can respond spontaneously across a wide demographic area. When the survey asks for sensitive information the whole standard of data acquisition changes, to effectively collect medical data a portfolio of remote methods should be leveraged.
For instance, chatbots have the advantage of delivering intervention and acute interactions. The other benefit of using a chatbot to collect a certain type of NLP medical knowledge is its capacity to replicate the interaction occurring between the patients and the doctors of other medical staff. Besides this with the data acquisition through chatbots, it is possible to develop the context around the specific medical domain. 

In the medical context, chatbot users are willing to disclose potentially sensitive information. Providing new opportunities to collect information on a broader range of outcomes generates higher-frequency data, and enables researchers with limited budgets to conduct larger-scale data collection.  One of the better approaches is to use mixed-mode data collection based on an exploratory analysis. Increasing the frequency of data collection is an important stage in designing an effective Natural Language processing pipeline. High-frequency data-gathering methods should be aligned with the lower-cost research approaches. 

Also to maximize response rates or reduce cost a mixed-mode data collection can be used to increase the response rate. Increasing the number of reliable respondents using audio-computer-assisted utilities can be highly beneficial. 

Knowing about the respondent's level of literacy as well as their level of comfort in the use of the proposed technology is a deterministic factor in designing a mixed-mode data collection basket. If this approach successfully transits into autonomous data gathering for each specific knowledge domain, then the initial stage in the whole structure of the natural language processing pipeline has a highly effective design. 


### Note-taking section 2 

One of the most valuable lessons we could learn from the pandemic was the importance of immediate medical knowledge gathering in diverse sub-categories based on their further uses. Once the data is collected it should enter into data fusion logic, data curation, data sharing, anomaly detection, data corrections. In this process their interventions of cumulative and daily feature requests. Through data service APIs the main preprocessing tasks such as anomaly detection and data recovery, multi-stage extraction, transform and load. An example of this is autonomous data collection of online dashboards from varied data sourcing such as national reporting websites, state and local reporting and global situation coverages. Then independent data fusion techniques are applied to the collected data to prepare it for the presenting insights in varied interpretational forms.  

Large-scale Public health-related data collection and reporting remains in cases where the diseases are relatively new and less known. There should be a standard applied to the data collecting procedure to ensure accuracy, transparency, and integrity (this needs expansion)
The pipeline goes through 4 steps and these are:
1)	Data sourcing which describes the identification and validation of trusted, open-source data sources. 
2)	Autonomous collection uses web scraping algorithms to collect raw data from open-source data sources. 
3)	Comprehensive data curation passes the data through several quality control mechanisms including an in-house designed anomaly detection service. In this stage, the data fusion services curate the cleaned data into a single production database
4)	Data sharing is the publication of production data into the online data interpretation tools  

For instance, social media posts from the official represent a good resource (is it in the nlp context). Based on the frequency of publishing the base for the medical NLP changes a challenge is to change the mapping of the resources. (what do you mean by mapping the resources?)

#### The data sourcing challenges  
One of the most difficult parts of acquiring medical NLP data for a specific domain is being able to identify definitions used for specific contexts, frequency of data production and availability, and the suitability of the source for manual or automated data collection. 

### Online data acquisition in the NLP pipeline with scrapping algorithms 
The autonomous data collection involves being able to collect data using web and data scrapper techniques and algorithms. The scrappers can be bonded directly to a dashboard in particular cases. Individual data scraping algorithms can be designed for a variety of resources. 
It should be noted that the maintenance level needed for scrapping methods is higher than regular procedural assets in the pipeline. Since a minor change in the data structure can influence the end reporting result. For instance, if the scrapper fails to collect the information needed then it should publish a notifying maintenance prompt. Finally, to have a diverse range of data that covers most aspects, it is suggested to have a regular update interval and fetch the data between these time spans. 
For example, in the case of online medical chatbots, we can Fetch data from them. This typically involves accessing the chatbot's backend system or database where the conversation logs and user inputs are stored. There are several ways to fetch this data:

1. API Integration: Many chatbot platforms provide APIs that allow developers to access conversation logs, user inputs, and other relevant data. By integrating with the chatbot's API, you can fetch the required data and use it for analysis or other purposes.

2. Database Access: If the chatbot's data is stored in a database, you can use database querying techniques to fetch the required information. This may involve writing SQL queries or using database access methods provided by the chatbot platform.

3. Exporting Logs: Some chatbot platforms allow administrators to export conversation logs and user data in a structured format such as CSV or JSON. This exported data can then be used for analysis or reporting.

4. Analytics Tools: Many chatbot platforms come with built-in analytics and reporting tools that allow you to analyze conversation data, user interactions, and other metrics. These tools can be used to fetch and analyze chatbot data.

### Medical Normative Language Dataset 


### The complexities of data curation 
The gathered data should be preprocessed and curated. On top of this sometimes there is a need for further data assessment such as checking the validity and examining the possibilities of anomalous source of data which bring about the ambiguities within the medical context. 



### Natural Language Processing Pipelines instances in the medical world 


### How the process of feature extraction is done? 

### Model Training and Evaluation


### Model Deployment 




### References for this post
[[1]](https://www.econstor.eu/bitstream/10419/265818/1/dp15597.pdf) Crowdsourcing a Normative Natural Language Dataset: A Comparison of Amazon Mechanical Turk and In-Lab Data Collection <br>
[[2]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9432867/) The Johns Hopkins University Center for Systems Science and Engineering COVID-19 Dashboard: data collection process, challenges faced, and lessons learned <br> 
[[3]]() Ask Rosa â€“ The making of a digital genetic conversation tool, a chatbot, about hereditary illnesses 
